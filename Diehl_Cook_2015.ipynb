{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting brian2\n",
      "  Downloading brian2-2.9.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.12/dist-packages (from brian2) (2.0.2)\n",
      "Requirement already satisfied: cython>=0.29.21 in /usr/local/lib/python3.12/dist-packages (from brian2) (3.0.12)\n",
      "Requirement already satisfied: sympy>=1.2 in /usr/local/lib/python3.12/dist-packages (from brian2) (1.13.3)\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from brian2) (3.2.3)\n",
      "Requirement already satisfied: jinja2>=2.7 in /usr/local/lib/python3.12/dist-packages (from brian2) (3.1.6)\n",
      "Requirement already satisfied: setuptools>=61 in /usr/local/lib/python3.12/dist-packages (from brian2) (75.2.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from brian2) (25.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2>=2.7->brian2) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.2->brian2) (1.3.0)\n",
      "Downloading brian2-2.9.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: brian2\n",
      "Successfully installed brian2-2.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install brian2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from brian2 import *\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from random import randrange, seed as rseed\n",
    "from struct import unpack\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "# 파라ㅏ미터 값 축소\n",
    "N_TRAIN = 3_000\n",
    "N_OBSERVE = 500\n",
    "N_TEST   = 300\n",
    "\n",
    "SEED = 42\n",
    "MNIST_PATH = Path('../mnist')   # MNIST IDX 파일 4개 있는 곳임\n",
    "DATA_PATH  = Path('data')\n",
    "\n",
    "N_SAVE_POINTS = 50\n",
    "\n",
    "# 건드리지 말라고 되어 있는 부분이긴 한ㄷ;\n",
    "# 빠르게 테스트 해야되니까 뉴런 개수만 좀 줄이기\n",
    "\n",
    "N_INP = 784\n",
    "N_NEURONS = 200\n",
    "V_EXC_REST = -65 * mV\n",
    "V_INH_REST = -60 * mV\n",
    "INTENSITY = 2\n",
    "\n",
    "# 시뮬 시간도 줄이기\n",
    "ACTIVE_T = 200 * ms             # 입력 자극 시간 줄이기 (350ms -> 200ms)\n",
    "REST_T   = 100 * ms             # 휴지 시간 줄이기 (150ms -> 100ms)\n",
    "defaultclock.dt = 1 * ms        # 해상도 완화\n",
    "\n",
    "# 측면 억제(고정 가중치)\n",
    "W_EXC_INH = 10.4\n",
    "W_INH_EXC = 17.0\n",
    "\n",
    "# (선택) 수동 모드 실행을 원하면 아래 값을 'train'/'observe'/'test'/'plot' 중 택1\n",
    "MODE = None  # None이면 자동 러너 사용\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_npy(arr, path):\n",
    "    arr = np.array(arr)\n",
    "    print('%-9s %-15s => %-30s' % ('Saving', arr.shape, path))\n",
    "    np.save(path, arr)\n",
    "\n",
    "def load_npy(path):\n",
    "    arr = np.load(path)\n",
    "    print('%-9s %-30s => %-15s' % ('Loading', path, arr.shape))\n",
    "    return arr\n",
    "\n",
    "def read_mnist(training: bool):\n",
    "    tag = 'train' if training else 't10k'\n",
    "    img_file = MNIST_PATH / f'{tag}-images-idx3-ubyte'\n",
    "    lbl_file = MNIST_PATH / f'{tag}-labels-idx1-ubyte'\n",
    "\n",
    "    if img_file.exists() and lbl_file.exists():\n",
    "        # ---- IDX 원본에서 직접 읽기 ----\n",
    "        with open(img_file, 'rb') as images, open(lbl_file, 'rb') as labels:\n",
    "            images.read(4)                              # magic\n",
    "            n_images = unpack('>I', images.read(4))[0]\n",
    "            n_rows   = unpack('>I', images.read(4))[0]\n",
    "            n_cols   = unpack('>I', images.read(4))[0]\n",
    "            labels.read(4)                              # magic\n",
    "            x = np.frombuffer(images.read(), dtype=np.uint8)\n",
    "            x = x.reshape(n_images, -1).astype(np.float32) / 8.0\n",
    "            y = np.frombuffer(labels.read(), dtype=np.uint8)\n",
    "        return x, y\n",
    "\n",
    "    # ---- 대체 경로: Keras MNIST 사용 ----\n",
    "    print(f\"MNIST로드 (MNIST_PATH={MNIST_PATH})\")\n",
    "    try:\n",
    "        try:\n",
    "            from tensorflow.keras.datasets import mnist as keras_mnist  # TF가 있으면 여기서\n",
    "        except Exception:\n",
    "            from keras.datasets import mnist as keras_mnist             # 또는 standalone keras\n",
    "    except Exception as e:\n",
    "        raise FileNotFoundError(\n",
    "        ) from e\n",
    "\n",
    "    (X_train, y_train), (X_test, y_test) = keras_mnist.load_data()\n",
    "    x, y = (X_train, y_train) if training else (X_test, y_test)\n",
    "    x = x.reshape(x.shape[0], -1).astype(np.float32) / 8.0\n",
    "    y = y.astype(np.uint8)\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 뉴런 막전위, 흥분성/억제성 전류, 시냅스 전도, 타이머 포함\n",
    "\n",
    "def build_network(training: bool):\n",
    "    # 공통 뉴런 방정식\n",
    "    eqs = '''\n",
    "    dv/dt = (v_rest - v + i_exc + i_inh) / tau_mem  : volt (unless refractory)\n",
    "    i_exc = ge * -v                         : volt\n",
    "    i_inh = gi * (v_inh_base - v)           : volt\n",
    "    dge/dt = -ge/(1 * ms)                   : 1\n",
    "    dgi/dt = -gi/(2 * ms)                   : 1\n",
    "    dtimer/dt = 1                           : second\n",
    "    '''\n",
    "    reset = f'v = {V_EXC_REST!r}; timer = 0 * ms'\n",
    "\n",
    "    # 흥분성 뉴런: 학습 시 theta 동적, 평가 시 저장된 theta 사용\n",
    "    if training:\n",
    "        exc_eqs = eqs + '\\n        dtheta/dt = -theta / (1e7 * ms) : volt\\n'\n",
    "        arr_theta = np.ones(N_NEURONS) * 20 * mV\n",
    "        reset += '; theta += 0.05 * mV'\n",
    "    else:\n",
    "        exc_eqs = eqs + '\\n        theta : volt\\n'\n",
    "        arr_theta = load_npy(DATA_PATH / 'theta.npy') * volt\n",
    "\n",
    "    exc_eqs = Equations(exc_eqs, tau_mem=100*ms, v_rest=V_EXC_REST, v_inh_base=-100*mV)\n",
    "    ng_exc = NeuronGroup(\n",
    "        N_NEURONS, exc_eqs,\n",
    "        threshold='v > (theta - 72 * mV) and (timer > 50 * ms)',\n",
    "        refractory=5*ms, reset=reset, method='euler', name='exc'\n",
    "    )\n",
    "    ng_exc.v = V_EXC_REST\n",
    "    ng_exc.theta = arr_theta\n",
    "\n",
    "    # 억제성 뉴런\n",
    "    inh_eqs = Equations(eqs, tau_mem=10*ms, v_rest=V_INH_REST, v_inh_base=-85*mV)\n",
    "    ng_inh = NeuronGroup(\n",
    "        N_NEURONS, inh_eqs,\n",
    "        threshold='v > -40 * mV', refractory=2*ms, reset='v = -45 * mV',\n",
    "        method='euler', name='inh'\n",
    "    )\n",
    "    ng_inh.v = V_INH_REST\n",
    "\n",
    "    # 측면 억제: exc→inh (1:1), inh→exc (자기 제외 전결합)\n",
    "    syns_exc_inh = Synapses(ng_exc, ng_inh, on_pre=f'ge_post += {W_EXC_INH}')\n",
    "    syns_exc_inh.connect(j='i')\n",
    "    syns_inh_exc = Synapses(ng_inh, ng_exc, on_pre=f'gi_post += {W_INH_EXC}')\n",
    "    syns_inh_exc.connect('i != j')\n",
    "\n",
    "    # 입력층(포아송 발화)\n",
    "    pg_inp = PoissonGroup(N_INP, 0*Hz, name='inp')\n",
    "\n",
    "    # 입력 -> 흥분성: 학습 시 STDP, 아니면 저장된 w 사용\n",
    "    model = 'w : 1'\n",
    "    on_pre = 'ge_post += w'\n",
    "    on_post = ''\n",
    "    if training:\n",
    "        on_pre  += '; pre = 1.; w = clip(w - 0.0001 * post1, 0, 1.0)'\n",
    "        on_post += 'post2bef = post2; w = clip(w + 0.01 * pre * post2bef, 0, 1.0); post1=1.; post2=1.'\n",
    "        model   += '''\n",
    "        post2bef : 1\n",
    "        dpre/dt   = -pre/(20*ms)  : 1 (event-driven)\n",
    "        dpost1/dt = -post1/(20*ms): 1 (event-driven)\n",
    "        dpost2/dt = -post2/(40*ms): 1 (event-driven)\n",
    "        '''\n",
    "        weights = (np.random.random(N_INP * N_NEURONS) + 0.01) * 0.3\n",
    "    else:\n",
    "        weights = load_npy(DATA_PATH / 'weights.npy')\n",
    "\n",
    "    syns_inp_exc = Synapses(pg_inp, ng_exc, model=model, on_pre=on_pre, on_post=on_post, name='inp_exc')\n",
    "    syns_inp_exc.connect(True)\n",
    "    syns_inp_exc.delay = 'rand() * 10 * ms'\n",
    "    syns_inp_exc.w = weights\n",
    "\n",
    "    # 모니터 + 네트워크 구성\n",
    "    exc_mon = SpikeMonitor(ng_exc, name='sp_exc')\n",
    "    net = Network([pg_inp, ng_exc, ng_inh, syns_inp_exc, syns_exc_inh, syns_inh_exc, exc_mon])\n",
    "    net.run(0*ms)  # 초기화\n",
    "    return net\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sample(net, sample, intensity):\n",
    "    # sample(784) -> 포아송 발화율로 200ms 제시, 100ms 휴지\n",
    "    exc_mon = net['sp_exc']\n",
    "    prev = exc_mon.count[:]\n",
    "    net['inp'].rates = sample * intensity * Hz\n",
    "    net.run(ACTIVE_T)\n",
    "    nextc = exc_mon.count[:]\n",
    "    net['inp'].rates = 0 * Hz\n",
    "    net.run(REST_T)\n",
    "    pat = nextc - prev\n",
    "    if np.sum(pat) < 5:  # 발화가 너무 적으면 강도 높여서 재시도 해보깅\n",
    "        return show_sample(net, sample, intensity + 1)\n",
    "    return pat\n",
    "\n",
    "def predict(groups, rates):\n",
    "    # 각 클래스 그룹의 평균 발화율을 비교해 argmax\n",
    "    return np.argmax([rates[grp].mean() for grp in groups])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_plastic_weights(syns):\n",
    "    # 열 합(각 뉴런으로 들어오는 총합)을 78로 정규화 -> 발산 방지/경쟁 안정화\n",
    "    conns = np.reshape(syns.w, (N_INP, N_NEURONS))\n",
    "    col_sums = np.sum(conns, axis=0)\n",
    "    factors = 78. / col_sums\n",
    "    conns *= factors\n",
    "    syns.w = conns.reshape(-1)\n",
    "\n",
    "def stats(net):\n",
    "    tick = defaultclock.timestep[:]\n",
    "    cnt  = np.sum(net['sp_exc'].count[:])\n",
    "    w    = net['inp_exc'].w\n",
    "    w_mu, w_std = np.mean(w), np.std(w)\n",
    "    theta = net['exc'].theta / mV\n",
    "    return [tick, cnt, w_mu, w_std, np.mean(theta), np.std(theta)]\n",
    "\n",
    "def train():\n",
    "    # STDP 학습\n",
    "    X, Y = read_mnist(True)\n",
    "    n_samples = X.shape[0]\n",
    "    net = build_network(True)\n",
    "\n",
    "    rows   = [stats(net) + [-1]]\n",
    "    w_hist = [np.array(net['inp_exc'].w)]\n",
    "    ratio  = max(N_TRAIN // N_SAVE_POINTS, 1)\n",
    "\n",
    "    for i in tqdm(range(N_TRAIN), desc=\"Train\", unit=\"img\", leave=True):\n",
    "        ix = i % n_samples\n",
    "        normalize_plastic_weights(net['inp_exc'])\n",
    "        show_sample(net, X[ix], INTENSITY)\n",
    "        rows.append(stats(net) + [Y[ix]])\n",
    "\n",
    "        if i % ratio == 0:\n",
    "            w_hist.append(np.array(net['inp_exc'].w))\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            t = rows[-1]\n",
    "            tqdm.write(f\"[Train] step={i+1}  w_mu={t[2]:.4f}  theta_mu={t[4]:.2f}mV\")\n",
    "\n",
    "    save_npy(rows,                 DATA_PATH / 'train_stats.npy')\n",
    "    save_npy(w_hist,               DATA_PATH / 'train_w_hist.npy')\n",
    "    save_npy(net['inp_exc'].w,     DATA_PATH / 'weights.npy')\n",
    "    save_npy(net['exc'].theta,     DATA_PATH / 'theta.npy')\n",
    "\n",
    "\n",
    "def observe():\n",
    "    # 클래스별 평균 반응 -> 뉴런별 최다 반응 클래스 매핑(assign.npy)\n",
    "    X, Y = read_mnist(True)\n",
    "    n_samples = X.shape[0]\n",
    "    net = build_network(False)\n",
    "\n",
    "    rows = [stats(net) + [-1]]\n",
    "    responses = defaultdict(list)\n",
    "\n",
    "    for i in tqdm(range(N_OBSERVE), desc=\"Observe\", unit=\"img\", leave=True):\n",
    "        ix = i % n_samples\n",
    "        exc = show_sample(net, X[ix], INTENSITY)\n",
    "        rows.append(stats(net) + [Y[ix]])\n",
    "        responses[Y[ix]].append(exc)\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            tqdm.write(f\"[Observe] step={i+1}\")\n",
    "\n",
    "    res = np.zeros((10, N_NEURONS))\n",
    "    for cls, vals in responses.items():\n",
    "        res[cls] = np.array(vals).mean(axis=0)\n",
    "\n",
    "    assign = np.argmax(res, axis=0)\n",
    "    save_npy(assign, DATA_PATH / 'assign.npy')\n",
    "    save_npy(rows,   DATA_PATH / 'observe_stats.npy')\n",
    "\n",
    "\n",
    "def test():\n",
    "    # assign.npy(뉴런 -> 클래스 매핑) 필요\n",
    "    conf   = np.zeros((10, 10))\n",
    "    assign = np.load(DATA_PATH / 'assign.npy')\n",
    "    groups = [np.where(assign == i)[0] for i in range(10)]\n",
    "\n",
    "    X, Y = read_mnist(False)\n",
    "    net  = build_network(False)\n",
    "    correct = 0\n",
    "\n",
    "    for i in tqdm(range(N_TEST), desc=\"Test\", unit=\"img\", leave=True):\n",
    "        ix = randrange(len(X))\n",
    "        exc = show_sample(net, X[ix], INTENSITY)\n",
    "        guess, real = predict(groups, exc), Y[ix]\n",
    "        if guess == real:\n",
    "            correct += 1\n",
    "        conf[real, guess] += 1\n",
    "\n",
    "        if (i + 1) % 10 == 0:\n",
    "            tqdm.write(f\"[Test] {i+1}/{N_TEST}  Acc={correct/(i+1):.3f}\")\n",
    "\n",
    "    final_acc = np.trace(conf) / np.sum(conf)\n",
    "    print('Final Accuracy: %6.3f' % final_acc)\n",
    "\n",
    "    conf = conf / conf.sum(axis=1, keepdims=True)\n",
    "    save_npy(conf, DATA_PATH / 'confusion.npy')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot():\n",
    "    conf = np.load(DATA_PATH / 'confusion.npy')\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    plt.imshow(100*conf, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    for i, j in itertools.product(range(conf.shape[0]), range(conf.shape[1])):\n",
    "        if conf[i, j] == 0: continue\n",
    "        plt.text(j, i, f\"{round(100*conf[i,j])}%\",\n",
    "                 ha='center', va='center',\n",
    "                 color='white' if conf[i, j] > 0.5 else 'black')\n",
    "    plt.colorbar()\n",
    "    plt.xticks(range(10)); plt.yticks(range(10))\n",
    "    plt.xlabel('Predicted label'); plt.ylabel('True label')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Auto] weights/theta 없음 -> train 실행\n",
      "[read_mnist] IDX 파일이 없어 Keras MNIST로 대체 로드합니다. (MNIST_PATH=../mnist)\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9a0ceae24d24abe8c10ca4be2c39a84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/3000 [00:00<?, ?img/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] step=100  w_mu=0.0995  theta_mu=20.43mV\n",
      "[Train] step=200  w_mu=0.0995  theta_mu=20.66mV\n",
      "[Train] step=300  w_mu=0.0995  theta_mu=20.89mV\n",
      "[Train] step=400  w_mu=0.0995  theta_mu=21.11mV\n",
      "[Train] step=500  w_mu=0.0995  theta_mu=21.32mV\n",
      "[Train] step=600  w_mu=0.0995  theta_mu=21.56mV\n",
      "[Train] step=700  w_mu=0.0995  theta_mu=21.82mV\n",
      "[Train] step=800  w_mu=0.0995  theta_mu=22.11mV\n",
      "[Train] step=900  w_mu=0.0995  theta_mu=22.39mV\n",
      "[Train] step=1000  w_mu=0.0995  theta_mu=22.61mV\n",
      "[Train] step=1100  w_mu=0.0995  theta_mu=22.90mV\n",
      "[Train] step=1200  w_mu=0.0995  theta_mu=23.27mV\n",
      "[Train] step=1300  w_mu=0.0995  theta_mu=23.90mV\n",
      "[Train] step=1400  w_mu=0.0995  theta_mu=24.58mV\n",
      "[Train] step=1500  w_mu=0.0995  theta_mu=24.79mV\n",
      "[Train] step=1600  w_mu=0.0995  theta_mu=25.00mV\n",
      "[Train] step=1700  w_mu=0.0995  theta_mu=25.17mV\n",
      "[Train] step=1800  w_mu=0.0995  theta_mu=25.47mV\n",
      "[Train] step=1900  w_mu=0.0995  theta_mu=25.78mV\n",
      "[Train] step=2000  w_mu=0.0995  theta_mu=26.14mV\n",
      "[Train] step=2100  w_mu=0.0995  theta_mu=26.79mV\n",
      "[Train] step=2200  w_mu=0.0995  theta_mu=27.00mV\n",
      "[Train] step=2300  w_mu=0.0995  theta_mu=27.23mV\n",
      "[Train] step=2400  w_mu=0.0995  theta_mu=27.41mV\n",
      "[Train] step=2500  w_mu=0.0995  theta_mu=27.58mV\n",
      "[Train] step=2600  w_mu=0.0995  theta_mu=27.90mV\n",
      "[Train] step=2700  w_mu=0.0995  theta_mu=28.08mV\n",
      "[Train] step=2800  w_mu=0.0995  theta_mu=28.36mV\n",
      "[Train] step=2900  w_mu=0.0995  theta_mu=28.54mV\n",
      "[Train] step=3000  w_mu=0.0995  theta_mu=28.72mV\n",
      "Saving    (3001, 7)       => data/train_stats.npy          \n",
      "Saving    (51, 156800)    => data/train_w_hist.npy         \n",
      "Saving    (156800,)       => data/weights.npy              \n",
      "Saving    (200,)          => data/theta.npy                \n",
      "[Auto] assign.npy 없음 -> observe 실행\n",
      "[read_mnist] IDX 파일이 없어 Keras MNIST로 대체 로드합니다. (MNIST_PATH=../mnist)\n",
      "Loading   data/theta.npy                 => (200,)         \n",
      "Loading   data/weights.npy               => (156800,)      \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6100c082c3b44a70a82f1f9ad527e6ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Observe:   0%|          | 0/500 [00:00<?, ?img/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Observe] step=100\n",
      "[Observe] step=200\n",
      "[Observe] step=300\n",
      "[Observe] step=400\n",
      "[Observe] step=500\n",
      "Saving    (200,)          => data/assign.npy               \n",
      "Saving    (501, 7)        => data/observe_stats.npy        \n",
      "[Auto] test 실행\n",
      "[read_mnist] IDX 파일이 없어 Keras MNIST로 대체 로드합니다. (MNIST_PATH=../mnist)\n",
      "Loading   data/theta.npy                 => (200,)         \n",
      "Loading   data/weights.npy               => (156800,)      \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36d1cc4277b945a0899a57d2118d421c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test:   0%|          | 0/300 [00:00<?, ?img/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 10/300  Acc=0.400\n",
      "[Test] 20/300  Acc=0.450\n",
      "[Test] 30/300  Acc=0.567\n",
      "[Test] 40/300  Acc=0.625\n",
      "[Test] 50/300  Acc=0.620\n",
      "[Test] 60/300  Acc=0.650\n",
      "[Test] 70/300  Acc=0.643\n",
      "[Test] 80/300  Acc=0.662\n",
      "[Test] 90/300  Acc=0.656\n",
      "[Test] 100/300  Acc=0.670\n",
      "[Test] 110/300  Acc=0.682\n",
      "[Test] 120/300  Acc=0.692\n",
      "[Test] 130/300  Acc=0.708\n",
      "[Test] 140/300  Acc=0.707\n",
      "[Test] 150/300  Acc=0.700\n",
      "[Test] 160/300  Acc=0.681\n",
      "[Test] 170/300  Acc=0.688\n",
      "[Test] 180/300  Acc=0.667\n",
      "[Test] 190/300  Acc=0.653\n",
      "[Test] 200/300  Acc=0.645\n",
      "[Test] 210/300  Acc=0.643\n",
      "[Test] 220/300  Acc=0.641\n",
      "[Test] 230/300  Acc=0.648\n",
      "[Test] 240/300  Acc=0.646\n",
      "[Test] 250/300  Acc=0.636\n",
      "[Test] 260/300  Acc=0.631\n",
      "[Test] 270/300  Acc=0.630\n",
      "[Test] 280/300  Acc=0.632\n",
      "[Test] 290/300  Acc=0.638\n",
      "[Test] 300/300  Acc=0.633\n",
      "Final Accuracy:  0.633\n",
      "Saving    (10, 10)        => data/confusion.npy            \n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    seed(SEED); rseed(SEED)\n",
    "    DATA_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if MODE is None:\n",
    "        need_train   = not (DATA_PATH / 'weights.npy').exists() or not (DATA_PATH / 'theta.npy').exists()\n",
    "        need_observe = not (DATA_PATH / 'assign.npy').exists()\n",
    "\n",
    "        if need_train:\n",
    "            print('[Auto] train 실행')\n",
    "            train()      # Train 0/3000\n",
    "        if need_observe:\n",
    "            print('[Auto] observe 실행')\n",
    "            observe()    # Observe 0/500\n",
    "        print('[Auto] test 실행')\n",
    "        test()          # Test 0/300\n",
    "    else:\n",
    "        cmds = dict(train=train, observe=observe, test=test, plot=plot)\n",
    "        print(f'[Manual] {MODE} 실행')\n",
    "        cmds[MODE]()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
